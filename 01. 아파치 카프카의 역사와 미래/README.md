# 아파치 카프카의 역사와 미래

## 카프카의 탄생
2011년 소셜 네트워크 사이트인 '링크드인(LinkedId)'에서 파편화된 데이터 수집 및 분배 아키텍쳐를 운영하는 데에 큰 어령움을 겪었다. 

시간이 지날수록 아키텍쳐는 거대해지고 소스 애플리케이션과 타깃 애플리케이션의 개수가 점점 많아지면서 문제가 생겼다. 데이터를 전송하는 라인이 기하급수적으로 복잡해지기 시작했다.

![image](https://user-images.githubusercontent.com/40031858/169800474-dd01145d-0606-4d5f-ae85-b585de6e04bb.png)

이를 해결하기 위해 기존에 ㄴ나와 있던 각종 상용 데이터 프레임워크와 오픈소스를 아키텍쳐에 녹여내어 데이터 파이프라인의 파편화를 개선하려고 했다. 다양한 메시징 플랫폼과 ETL 툴을

적용해서 아키텍쳐를 변경하려고 노력했지만 파편화된 데이터 파이프라인의 복잡도를 낮춰주는 아키텍쳐는 되지 못했다.

![image](https://user-images.githubusercontent.com/40031858/169800638-1a1a7635-c674-46e8-90a3-6e278bdde286.png)

링크드인의 데이터팀은 신규 시스템을 만들기로 결정했고 그 결과물이 `아프카 카프카`다. 링크드인의 내부 데이터 흐름을 개선하기 위해

개발한 카프카는 각각의 애플리케이션끼리 연결하여 데이터를 처리하는 것이 아니라 한 곳에 모아 처리할 수 있도록 중앙집중화했다.

![image](https://user-images.githubusercontent.com/40031858/169800820-73291212-d1c2-4395-9548-a95c568ef263.png)

기존에 1:1 매칭으로 개발하고 운영하던 데이터 파이프라인은 커플링으로 인해 한쪽의 이슈가 다른 한쪽의 애플리케이션에 영향을 미치곤 했지만, 카프카는 이러한 의존도를 타파했다.

이제 소스애플리케이션에서 생성되는 데이터는 어느 타깃 애플리케이션으로 보낼 것인지 고민하지 않고 카프카로 넣으면 된다.

카프카 내부에 데이터가 저장되는 파티션의 동작은 `FIFO`방식의 큐 자료구조와 유사하다.

큐에 데이터를 보내는 것이 `프로듀서`, 큐에서 데이터를 가져가는 것이 `컨슈머`다.

### 빅데이터 파이프라인에 적합한 카프카의 특징 4가지
- `높은 처리량`
  - 많은 양의 데이터를 묶음 단위로 처리하는 배치로 빠르게 처리할 수 있기 때문에 대용량의 실시간 로그데이터를 처리하는데 적합
  - 추가적으로 파티션 단위를 통해 동일 목적의 데이터를 여러 파티션에 분배하고 데이터를 병렬 처리할 수 있다.
- `확장성`
  - 데이터가 적을 때는 카프카 클러스터의 브로커를 최소한의 개수로 운영하다가 데이터가 많아지면 클러스터의 브로커 개수를 장연스럽게 늘려 `스케일 아웃` 할 수 있다.
  - 반대로 데이터 개수가 적어지고 추가 서버들이 더는 필요없어지면 브로커 개수를 줄여 `스케일 인` 할 수 있다.
- `영속성`
  - 카프카는 전송받은 데이터를 메모리에 저장하지 않고 파일시스템에 저장한다. 카프카는 운영체제 레벨에서 `페이지 캐시` 메모리 영역을 사용하여 한번 읽은 파일 내용은 메모리에 저장시켰다가 다시 사용하는 방식
- `고가용성`
  - 클러스터로 이루어진 카프카는 데이터의 복제를 통해 고가용성의 특징을 가지게 되었다. 프로듀서로 전송받은 데이터를 여러 브로커 중 1대의 브로커에만
  - 저장하는 것이 아니라 또 다른 브로커에도 저장하는 것이다. 

|||
|:--:|:--:|
|배치 데이터|스트림 데이터|
|- 한정된 데이터 처리<br/> - 대규모 배치 데이터를 위한 분산 처리 수행 <br/> - 분, 시간, 일 단위 처리를 위한 지연 발생 <br/> - 복잡한 키 조인 수행| - 무한 데이터 처리 <br/> - 지속적으로 들어오는 데이터를 위한 분산 처리 수행 </br> - 분 단위 이하 지연 발생 <br/> - 단순한 키 조인 수행|
